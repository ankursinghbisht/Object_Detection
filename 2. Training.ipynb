{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8129ea",
   "metadata": {},
   "source": [
    "# Setting up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the os module for operating system-related functionality\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model name for the project\n",
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet'\n",
    "\n",
    "# Pretrained model details\n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "\n",
    "# Script for generating TFRecord\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "\n",
    "# Label map file name\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing various paths used in the project\n",
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFJS_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
    "    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing file paths\n",
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directories if they don't exist\n",
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        !mkdir {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ceab0b",
   "metadata": {},
   "source": [
    "# Cloning Tensorflow Pretrained models ( TF Model Zoo )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility module for downloading files from the internet.\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the directory containing TensorFlow object detection code does not exist\n",
    "if not os.path.exists(os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection')):\n",
    "    \n",
    "    # Clone the TensorFlow models repository from GitHub into the specified path\n",
    "    !git clone https://github.com/tensorflow/models {paths['APIMODEL_PATH']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for downloading the Protocol Buffers compiler zip file\n",
    "url = \"https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip\"\n",
    "\n",
    "# Download the Protocol Buffers compiler zip file\n",
    "wget.download(url)\n",
    "\n",
    "# Move the downloaded file to the specified PROTOC_PATH\n",
    "!move protoc-3.15.6-win64.zip {paths['PROTOC_PATH']}\n",
    "\n",
    "# Navigate to the PROTOC_PATH and extract the contents of the zip file\n",
    "!cd {paths['PROTOC_PATH']} && tar -xf protoc-3.15.6-win64.zip\n",
    "\n",
    "# Add the bin directory of the Protocol Buffers compiler to the PATH environment variable\n",
    "os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(paths['PROTOC_PATH'], 'bin'))\n",
    "\n",
    "# Navigate to the TensorFlow models research directory and compile Protocol Buffers proto files\n",
    "!cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "# Copy setup.py file from object_detection/packages/tf2 to the research directory\n",
    "# Build and install the object_detection package\n",
    "!cd Tensorflow/models/research && copy object_detection\\\\packages\\\\tf2\\\\setup.py setup.py && python setup.py build && python setup.py install\n",
    "\n",
    "# Navigate to the slim directory and install the slim package\n",
    "!cd Tensorflow/models/research/slim && pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68184e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "# Verify Installation\n",
    "!python {VERIFICATION_SCRIPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing modules absent while verifing files ( missing modules are variable with each installation )\n",
    "\n",
    "!pip install tensorflow-gpu==2.10.0\n",
    "!pip install tensorflow==2.10.0\n",
    "!pip install pyyaml\n",
    "!pip install matplotlib\n",
    "!pip install protobuf==3.20.0\n",
    "!pip install pytz\n",
    "!pip install tzdata==2022.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caab55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained model from the specified URL\n",
    "wget.download(PRETRAINED_MODEL_URL)\n",
    "\n",
    "# Move the downloaded tar.gz file to the PRETRAINED_MODEL_PATH\n",
    "!move {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "\n",
    "# Navigate to the PRETRAINED_MODEL_PATH and extract the contents of the tar.gz file\n",
    "!cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55077d64",
   "metadata": {},
   "source": [
    "# Creating Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of labels, each with a name and an ID\n",
    "labels = [{'name':'Face', 'id':1},{'name':'ThumbsUp', 'id':2},{'name':'ThumbsDown', 'id':3},\n",
    "          {'name':'Hello', 'id':4},{'name':'ThankYou', 'id':5}, {'name':'Yes', 'id':6},\n",
    "          {'name':'No', 'id':7}, {'name':'LiveLong', 'id':8}]\n",
    "\n",
    "# Open the label map file for writing\n",
    "with open(files['LABELMAP'], 'w') as f:\n",
    "    # Iterate over each label and write the corresponding entries in the label map\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')               # Write the start of an item entry\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))  # Write the class name\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))          # Write the class ID\n",
    "        f.write('}\\n')                    # Write the end of the item entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c249de",
   "metadata": {},
   "source": [
    "# Creating Tensorflow records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file generate_tfrecord.py does not exist\n",
    "if not os.path.exists(files['TF_RECORD_SCRIPT']):\n",
    "    \n",
    "    # Clone the specified GitHub repository into the SCRIPTS_PATH\n",
    "    !git clone https://github.com/ankursinghbisht/GenerateTFRecord {paths['SCRIPTS_PATH']} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7afabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the generate_tfrecord.py script for training data\n",
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'train')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')}\n",
    "# Run the generate_tfrecord.py script for testing data\n",
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'test')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5dd9b1",
   "metadata": {},
   "source": [
    "# Copying Model Config to Training Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbe871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the 'pipeline.config' file from the pretrained model directory to the checkpoint directory\n",
    "!copy {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fe448",
   "metadata": {},
   "source": [
    "# 5. Updating Config For Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing pipeline configuration from the file\n",
    "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the pipeline configuration file and merge it with the protobuf message\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:\n",
    "    proto_str = f.read()\n",
    "    text_format.Merge(proto_str, pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify specific parameters in the pipeline configuration for transfer learning\n",
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path = files['LABELMAP']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the modified pipeline configuration back to a text format\n",
    "config_text = text_format.MessageToString(pipeline_config)\n",
    "\n",
    "# Write the updated configuration back to the pipeline configuration file\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:\n",
    "    f.write(config_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ObjectDetection",
   "language": "python",
   "name": "objectdetection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
